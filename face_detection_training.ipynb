{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636b70b7",
   "metadata": {},
   "source": [
    "# Face Detection Training and Inference\n",
    "Complete pipeline for training YOLOv8 face detector on WIDER FACE dataset and generating face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f144285",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade ultralytics facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7c624",
   "metadata": {},
   "source": [
    "## Parse WIDER FACE Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wider_annotations(annotation_file, img_base_path):\n",
    "    \"\"\"Parse WIDER FACE annotation file format\"\"\"\n",
    "    annotations = []\n",
    "    \n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < len(lines):\n",
    "        img_path = lines[idx].strip()\n",
    "        idx += 1\n",
    "        \n",
    "        if idx >= len(lines):\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            num_faces = int(lines[idx].strip())\n",
    "            idx += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        boxes = []\n",
    "        if num_faces > 0:\n",
    "            for _ in range(num_faces):\n",
    "                if idx >= len(lines):\n",
    "                    break\n",
    "                parts = lines[idx].strip().split()\n",
    "                if len(parts) >= 4:\n",
    "                    try:\n",
    "                        x, y, w, h = map(int, parts[:4])\n",
    "                        if w > 0 and h > 0:\n",
    "                            boxes.append([x, y, w, h])\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                idx += 1\n",
    "        \n",
    "        if boxes:\n",
    "            full_img_path = os.path.join(img_base_path, img_path)\n",
    "            if os.path.exists(full_img_path):\n",
    "                annotations.append({\n",
    "                    'image': full_img_path,\n",
    "                    'boxes': boxes\n",
    "                })\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "train_annotations = parse_wider_annotations(\n",
    "    '/kaggle/input/datasets/rahulftz/face-detection-dataset0906/wider_face_annotations/wider_face_split/wider_face_train_bbx_gt.txt',\n",
    "    '/kaggle/input/datasets/rahulftz/face-detection-dataset0906/WIDER_train/WIDER_train/images'\n",
    ")\n",
    "\n",
    "val_annotations = parse_wider_annotations(\n",
    "    '/kaggle/input/datasets/rahulftz/face-detection-dataset0906/wider_face_annotations/wider_face_split/wider_face_val_bbx_gt.txt',\n",
    "    '/kaggle/input/datasets/rahulftz/face-detection-dataset0906/WIDER_val/WIDER_val/images'\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_annotations)}')\n",
    "print(f'Validation samples: {len(val_annotations)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88110a3c",
   "metadata": {},
   "source": [
    "## Convert to YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(annotations, output_dir, split='train'):\n",
    "    \"\"\"Convert WIDER FACE annotations to YOLO format\"\"\"\n",
    "    img_dir = os.path.join(output_dir, 'images', split)\n",
    "    label_dir = os.path.join(output_dir, 'labels', split)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    \n",
    "    valid_samples = []\n",
    "    \n",
    "    for idx, ann in enumerate(annotations):\n",
    "        try:\n",
    "            img = cv2.imread(ann['image'])\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            new_img_name = f'{split}_{idx:06d}.jpg'\n",
    "            new_img_path = os.path.join(img_dir, new_img_name)\n",
    "            cv2.imwrite(new_img_path, img)\n",
    "            \n",
    "            label_file = os.path.join(label_dir, f'{split}_{idx:06d}.txt')\n",
    "            with open(label_file, 'w') as f:\n",
    "                for box in ann['boxes']:\n",
    "                    x, y, bw, bh = box\n",
    "                    x_center = (x + bw / 2) / w\n",
    "                    y_center = (y + bh / 2) / h\n",
    "                    box_w = bw / w\n",
    "                    box_h = bh / h\n",
    "                    \n",
    "                    x_center = max(0, min(1, x_center))\n",
    "                    y_center = max(0, min(1, y_center))\n",
    "                    box_w = max(0, min(1, box_w))\n",
    "                    box_h = max(0, min(1, box_h))\n",
    "                    \n",
    "                    f.write(f'0 {x_center:.6f} {y_center:.6f} {box_w:.6f} {box_h:.6f}\\n')\n",
    "            \n",
    "            valid_samples.append(new_img_name)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return valid_samples\n",
    "\n",
    "yolo_dataset_path = '/kaggle/working/yolo_dataset'\n",
    "os.makedirs(yolo_dataset_path, exist_ok=True)\n",
    "\n",
    "print('Converting training set...')\n",
    "train_samples = convert_to_yolo_format(train_annotations[:5000], yolo_dataset_path, 'train')\n",
    "print(f'Converted {len(train_samples)} training images')\n",
    "\n",
    "print('Converting validation set...')\n",
    "val_samples = convert_to_yolo_format(val_annotations[:1000], yolo_dataset_path, 'val')\n",
    "print(f'Converted {len(val_samples)} validation images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d350d5c",
   "metadata": {},
   "source": [
    "## Create YOLO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = {\n",
    "    'path': yolo_dataset_path,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'face'},\n",
    "    'nc': 1\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(yolo_dataset_path, 'data.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_config, f)\n",
    "\n",
    "print(f'YOLO config saved to {yaml_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960cc17",
   "metadata": {},
   "source": [
    "## Train YOLOv8 Face Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1eeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='face_detector',\n",
    "    project='/kaggle/working',\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    patience=10,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54003e68",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddffe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = '/kaggle/working/face_detector/weights/best.pt'\n",
    "output_model_path = '/kaggle/working/face_detector_best.pt'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    shutil.copy(best_model_path, output_model_path)\n",
    "    print(f'Model saved to {output_model_path}')\n",
    "else:\n",
    "    print('Best model not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ec056",
   "metadata": {},
   "source": [
    "## Load Trained Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = YOLO(output_model_path if os.path.exists(output_model_path) else 'yolov8n.pt')\n",
    "print('Face detector loaded')\n",
    "\n",
    "embedding_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "print('Embedding model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554a57f",
   "metadata": {},
   "source": [
    "## Face Detection and Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bfb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_embed_faces(image_path, output_dir='/kaggle/working/detected_faces'):\n",
    "    \"\"\"\n",
    "    Detect faces, crop them, and generate embeddings\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        output_dir: Directory to save cropped faces\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing face count, face paths, and embeddings\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return {'error': 'Failed to load image'}\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = face_detector(img_rgb, conf=0.5)\n",
    "    \n",
    "    face_crops = []\n",
    "    embeddings = []\n",
    "    face_paths = []\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        boxes = result.boxes\n",
    "        for box_idx, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "            \n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(img.shape[1], x2), min(img.shape[0], y2)\n",
    "            \n",
    "            if x2 > x1 and y2 > y1:\n",
    "                face_crop = img_rgb[y1:y2, x1:x2]\n",
    "                \n",
    "                face_filename = f'face_{idx}_{box_idx}.jpg'\n",
    "                face_path = os.path.join(output_dir, face_filename)\n",
    "                cv2.imwrite(face_path, cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))\n",
    "                face_paths.append(face_path)\n",
    "                \n",
    "                face_pil = Image.fromarray(face_crop)\n",
    "                face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    embedding = embedding_model(face_tensor)\n",
    "                    embedding = embedding.cpu().numpy().flatten()\n",
    "                    embedding = embedding / np.linalg.norm(embedding)\n",
    "                    embeddings.append(embedding.tolist())\n",
    "    \n",
    "    results_dict = {\n",
    "        'num_faces': len(face_paths),\n",
    "        'face_paths': face_paths,\n",
    "        'embeddings': embeddings\n",
    "    }\n",
    "    \n",
    "    embeddings_file = os.path.join(output_dir, 'embeddings.json')\n",
    "    with open(embeddings_file, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    \n",
    "    print(f'Detected {len(face_paths)} faces')\n",
    "    print(f'Cropped faces saved to {output_dir}')\n",
    "    print(f'Embeddings saved to {embeddings_file}')\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0db2d2",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dirs = [\n",
    "    '/kaggle/input/wider-face/WIDER_val/WIDER_val/images/0--Parade',\n",
    "    '/kaggle/input/wider-face/WIDER_val/WIDER_val/images/1--Handshaking',\n",
    "    '/kaggle/input/wider-face/WIDER_val/WIDER_val/images/12--Group'\n",
    "]\n",
    "\n",
    "for test_dir in test_image_dirs:\n",
    "    if os.path.exists(test_dir):\n",
    "        test_images = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.jpg')][:1]\n",
    "        break\n",
    "\n",
    "if test_images:\n",
    "    test_image = test_images[0]\n",
    "    print(f'Testing on: {test_image}')\n",
    "    \n",
    "    results = detect_and_embed_faces(test_image)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Number of faces detected: {results['num_faces']}\")\n",
    "    print(f\"Embedding dimension: {len(results['embeddings'][0]) if results['embeddings'] else 0}\")\n",
    "    print(f\"First embedding (truncated): {results['embeddings'][0][:10] if results['embeddings'] else None}\")\n",
    "else:\n",
    "    print('No test images found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8c52a",
   "metadata": {},
   "source": [
    "## Visualize Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1831863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_detections(image_path, output_path='/kaggle/working/detection_result.jpg'):\n",
    "    \"\"\"Visualize face detection results\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = face_detector(img_rgb, conf=0.5)\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            conf = float(box.conf[0])\n",
    "            cv2.putText(img_rgb, f'{conf:.2f}', (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Detected Faces: {len(boxes)}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Visualization saved to {output_path}')\n",
    "\n",
    "if test_images:\n",
    "    visualize_detections(test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7088a0",
   "metadata": {},
   "source": [
    "## Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_batch(image_paths, output_base_dir='/kaggle/working/batch_results'):\n",
    "    \"\"\"Process multiple images and generate embeddings for all faces\"\"\"\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        print(f'Processing {idx+1}/{len(image_paths)}: {img_path}')\n",
    "        \n",
    "        output_dir = os.path.join(output_base_dir, f'image_{idx:04d}')\n",
    "        results = detect_and_embed_faces(img_path, output_dir)\n",
    "        \n",
    "        all_results.append({\n",
    "            'image_path': img_path,\n",
    "            'results': results\n",
    "        })\n",
    "    \n",
    "    summary_file = os.path.join(output_base_dir, 'batch_summary.json')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    total_faces = sum(r['results']['num_faces'] for r in all_results)\n",
    "    print(f'\\nBatch processing completed!')\n",
    "    print(f'Total images processed: {len(image_paths)}')\n",
    "    print(f'Total faces detected: {total_faces}')\n",
    "    print(f'Summary saved to {summary_file}')\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if len(test_images) > 0:\n",
    "    batch_results = process_image_batch(test_images[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3ded3",
   "metadata": {},
   "source": [
    "## Export Functions for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionSystem:\n",
    "    \"\"\"Complete face detection and embedding system\"\"\"\n",
    "    \n",
    "    def __init__(self, detector_path, device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.face_detector = YOLO(detector_path)\n",
    "        self.embedding_model = InceptionResnetV1(pretrained='vggface2').eval().to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((160, 160)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        print(f'System initialized on {self.device}')\n",
    "    \n",
    "    def detect_faces(self, image_path, conf_threshold=0.5):\n",
    "        \"\"\"Detect faces in image\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_detector(img_rgb, conf=conf_threshold)\n",
    "        \n",
    "        detections = []\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                conf = float(box.conf[0])\n",
    "                detections.append({\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'confidence': conf\n",
    "                })\n",
    "        \n",
    "        return detections, img_rgb\n",
    "    \n",
    "    def generate_embedding(self, face_crop):\n",
    "        \"\"\"Generate embedding for a face crop\"\"\"\n",
    "        face_pil = Image.fromarray(face_crop)\n",
    "        face_tensor = self.transform(face_pil).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_model(face_tensor)\n",
    "            embedding = embedding.cpu().numpy().flatten()\n",
    "            embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def process_image(self, image_path, output_dir=None):\n",
    "        \"\"\"Complete pipeline: detect faces and generate embeddings\"\"\"\n",
    "        detections, img_rgb = self.detect_faces(image_path)\n",
    "        \n",
    "        results = {\n",
    "            'image_path': image_path,\n",
    "            'num_faces': len(detections),\n",
    "            'faces': []\n",
    "        }\n",
    "        \n",
    "        for idx, det in enumerate(detections):\n",
    "            x1, y1, x2, y2 = det['bbox']\n",
    "            face_crop = img_rgb[y1:y2, x1:x2]\n",
    "            \n",
    "            if face_crop.size > 0:\n",
    "                embedding = self.generate_embedding(face_crop)\n",
    "                \n",
    "                face_data = {\n",
    "                    'bbox': det['bbox'],\n",
    "                    'confidence': det['confidence'],\n",
    "                    'embedding': embedding.tolist()\n",
    "                }\n",
    "                \n",
    "                if output_dir:\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    face_path = os.path.join(output_dir, f'face_{idx}.jpg')\n",
    "                    cv2.imwrite(face_path, cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))\n",
    "                    face_data['saved_path'] = face_path\n",
    "                \n",
    "                results['faces'].append(face_data)\n",
    "        \n",
    "        return results\n",
    "\n",
    "system = FaceDetectionSystem(output_model_path if os.path.exists(output_model_path) else 'yolov8n.pt')\n",
    "\n",
    "if test_images:\n",
    "    demo_results = system.process_image(test_images[0], '/kaggle/working/system_output')\n",
    "    print(f\"System detected {demo_results['num_faces']} faces\")\n",
    "    print(f\"Embedding dimensions: {len(demo_results['faces'][0]['embedding']) if demo_results['faces'] else 0}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
